{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "import csv\n",
    "import  tqdm\n",
    " \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I will be creating a dataloader which has the images that we want to reconstruct. \n",
    "#We can also pass singluar images to the networks after converting then to tensors and normalizing them.\n",
    "#We could have also made a testloader which is the more common and logical way to do it.\n",
    "#But if you have a bunch of photos of some friends or something, you can keep them in a folder and make a dataloader as I have done here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_set(Dataset):\n",
    "    def __init__(self , csv_file  , root_dir , transform = None ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations) \n",
    "    \n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        try:\n",
    "            img_path = os.path.join(self.root_dir , self.annotations.iloc[index , 0])\n",
    "            image = Image.open(img_path )\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "      \n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        return image\n",
    "\n",
    "batch_size = 1\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "my_transforms = transforms.Compose([\n",
    "    #transforms.ToPILImage() , \n",
    "    transforms.Resize((64,64)) , \n",
    "    transforms.ToTensor() , \n",
    "    transforms.Normalize((0.0 , 0.0 , 0.0), (1.0 , 1.0 , 1.0)),\n",
    "])\n",
    "\n",
    "data=[]\n",
    "i = 0\n",
    "with open('test_image_annotation.csv', 'w', newline='') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    \n",
    "    for filename in os.listdir(r\"C:\\Users\\Abhrant\\Desktop\\abhrant\\work\\DEEP_LEARNING\\FaceDataset\"):\n",
    "        \n",
    "            data.append(filename)\n",
    "            writer.writerow(data)\n",
    "            data=[]\n",
    "            i += 1\n",
    "            if i >= 100:\n",
    "                break\n",
    "writeFile.close()\n",
    "\n",
    "colored_dataset = data_set(csv_file = \"test_image_annotation.csv\" , root_dir = r\"C:\\Users\\Abhrant\\Desktop\\abhrant\\work\\DEEP_LEARNING\\FaceDataset\" , \n",
    "                          transform = my_transforms)\n",
    "\n",
    "\n",
    "\n",
    "test_image_loader = DataLoader(dataset = colored_dataset , collate_fn = collate_fn ,  batch_size = batch_size , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(r\"C:\\Users\\Abhrant\\Desktop\\abhrant\\work\\DEEP_LEARNING\\vae_weight.pt\" , \n",
    "             map_location = 'cuda:0')\n",
    "vae.load_state_dict(state_dict)\n",
    "vae.eval()\n",
    "vae.cuda()\n",
    "\n",
    "for i in (test_image_loader):\n",
    "    \n",
    "    input_img = i\n",
    "    output , mu , logvar = vae.forward(i.to(\"cuda\"))\n",
    "    \n",
    "    recon_img = output.to(\"cpu\")\n",
    "    recon_img = recon_img.view(3 , 64 , 64)\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    recon_img = to_pil(recon_img)\n",
    "    \n",
    "    input_img = input_img.to(\"cpu\")\n",
    "    input_img = input_img.view(3 , 64 , 64)\n",
    "    input_img = to_pil(input_img)\n",
    "    \n",
    "    display(input_img , recon_img)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
