{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since this is a variational autoencoder, we will have a Flatten class to flatten the final feature map into a 1D vector\n",
    "#And a Unflatten class to change the latent vector z into the feature maps again.\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self , input):\n",
    "        return input.view(input.size(0) , -1)\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    def __init__(self , channel , height , width):\n",
    "        super(Unflatten , self).__init__()\n",
    "        \n",
    "        self.channel = channel\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "          \n",
    "    def forward(self , input):\n",
    "        return input.view(input.size(0) , self.channel , self.height , self.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variational Autoencoders work best when the weights of the various layers are initialized between -0.08 and 0.08.\n",
    "#So here we sample the weights of the layers from a uniform distribution having upper bound = 0.08 and lower bound = -0.08\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.uniform_(m.weight, -0.08 , 0.08)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.uniform_(m.weight, -0.08 , 0.08)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    if classname.find('ConvTranspose') != -1:\n",
    "        torch.nn.init.uniform_(m.weight, -0.08 , 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self , latent_vector_size):\n",
    "        super(convVAE , self).__init__()\n",
    "        \n",
    "        self.latent_vector_size = latent_vector_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "             \n",
    "            nn.Conv2d(3 , 32 , 3 , 1 , 1) ,\n",
    "            nn.ReLU() , \n",
    "            nn.MaxPool2d(2 , 2) , \n",
    "            nn.ReLU() ,\n",
    "            nn.BatchNorm2d(32) ,\n",
    "             \n",
    "            \n",
    "            nn.Conv2d(32 , 64 , 3 , 1 , 1) , \n",
    "            nn.ReLU() , \n",
    "            nn.MaxPool2d(2 , 2) , \n",
    "            nn.ReLU() ,\n",
    "            nn.BatchNorm2d(64) ,\n",
    "            \n",
    "            nn.Conv2d(64 , 128 , 3 , 1 , 1) , \n",
    "            nn.ReLU() , \n",
    "            nn.MaxPool2d(2 , 2) ,\n",
    "            nn.ReLU() ,\n",
    "            nn.BatchNorm2d(128) , \n",
    "            \n",
    "            nn.Conv2d(128 , 256 , 3 , 1 , 1) , \n",
    "            nn.ReLU() , \n",
    "            nn.MaxPool2d(2 , 2) , \n",
    "            nn.ReLU() , \n",
    "            nn.BatchNorm2d(256) , \n",
    "            \n",
    "            Flatten() , \n",
    "            nn.Linear(4096 , 1024) ,\n",
    "            nn.Linear(1024 , 32) ,\n",
    "            nn.ReLU()            \n",
    "            )\n",
    "        \n",
    "        self.mu = nn.Linear(32 , self.latent_vector_size)\n",
    "        self.logvar = nn.Linear(32 , self.latent_vector_size)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_size , 1024) , \n",
    "            nn.ReLU() , \n",
    "            nn.Linear(1024 , 4096) , \n",
    "            nn.ReLU() , \n",
    "            Unflatten(256 , 4 , 4) , \n",
    "           \n",
    "            nn.ConvTranspose2d(256 , 128 , 2 , 2) , \n",
    "            nn.ReLU() , \n",
    "            nn.BatchNorm2d(128) , \n",
    "            \n",
    "            nn.ConvTranspose2d(128 , 64 , 2 , 2) , \n",
    "            nn.ReLU() , \n",
    "            nn.BatchNorm2d(64) ,\n",
    "            \n",
    "            nn.ConvTranspose2d(64 , 32 , 2 , 2) , \n",
    "            nn.ReLU() , \n",
    "            nn.BatchNorm2d(32) ,\n",
    "            \n",
    "            nn.ConvTranspose2d(32 , 3 , 2 , 2) , \n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "    def encode(self , x):\n",
    "        h = self.encoder(x)\n",
    "        mu , logvar = self.mu(h) , self.logvar(h)\n",
    "        return mu , logvar            \n",
    "        \n",
    "    def reparameterize(self , mu , logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu +eps * std\n",
    "        return z\n",
    "        \n",
    "    def decode(self , z):\n",
    "        decoded_z = self.decoder(z)\n",
    "        return decoded_z\n",
    "        \n",
    "    def forward(self , x):\n",
    "        mu , logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu , logvar)\n",
    "        return self.decode(z) , mu , logvar\n",
    "            \n",
    "vae = convVAE(32)\n",
    "vae.to(\"cuda\")\n",
    "vae.apply(weights_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
